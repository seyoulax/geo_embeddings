{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1eef1b3-fb9e-4337-bbb3-a531cd0f028f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T12:01:51.985018Z",
     "iopub.status.busy": "2024-07-11T12:01:51.984012Z",
     "iopub.status.idle": "2024-07-11T12:01:53.710275Z",
     "shell.execute_reply": "2024-07-11T12:01:53.709490Z",
     "shell.execute_reply.started": "2024-07-11T12:01:51.984979Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -q -f https://data.pyg.org/whl/torch-2.0.1+cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46c3f0e5-98f7-4bc6-ab79-139e39bb24bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:33:15.812005Z",
     "iopub.status.busy": "2024-07-21T10:33:15.811019Z",
     "iopub.status.idle": "2024-07-21T10:33:15.826224Z",
     "shell.execute_reply": "2024-07-21T10:33:15.825356Z",
     "shell.execute_reply.started": "2024-07-21T10:33:15.811964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, groupby\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch_geometric.data import Batch\n",
    "from tqdm.notebook import tqdm\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be12822f-2548-423d-b87d-5d57d637c29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:33:16.072030Z",
     "iopub.status.busy": "2024-07-21T10:33:16.070815Z",
     "iopub.status.idle": "2024-07-21T10:33:16.089042Z",
     "shell.execute_reply": "2024-07-21T10:33:16.088207Z",
     "shell.execute_reply.started": "2024-07-21T10:33:16.071973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed for splitting\n",
    "SEED = 111\n",
    "\n",
    "# Select target region\n",
    "region = 473\n",
    "regions_mapper = {\n",
    "        777 : \"Moscow\",\n",
    "        812 : \"Saint-Petersburg\",\n",
    "        287 : \"Kazan\",\n",
    "        473 : \"Sochi\"\n",
    "    }\n",
    "\n",
    "# Select target bussiness\n",
    "target_business=\"kafe_delivery\"\n",
    "\n",
    "# Root paths for data\n",
    "root_data_path = \"/home/jupyter/datasphere/s3/s3-sirius/sirius_2024_participants/data/\"\n",
    "root_features_path = \"geo_features/\"\n",
    "root_transaction_path = \"transaction_data/\"\n",
    "root_graphs_path = \"/home/jupyter/datasphere/s3/s3-sirius/sirius_2024_participants/catboost_train/data/graphs/\"\n",
    "root_metro_features_path = \"/home/jupyter/datasphere/project/data/mrt/\"\n",
    "\n",
    "# Select features to delete\n",
    "cols2drop = [\"datep\"]\n",
    "\n",
    "# Select features to keep from transactions df\n",
    "transaction_features = [\"industry_type\", \"transaction_sum\", \"transaction_count\", \"customer_count\"]\n",
    "\n",
    "# Select name of zone_id column from your graph\n",
    "zone_id_column = \"zone_id\"\n",
    "\n",
    "# This dictionary with funcs to aggregate not target bussiness transactions features\n",
    "bussiness_features_agg = dict(\n",
    "    transaction_sum_sum=(\"transaction_sum\", \"sum\"),\n",
    "    transaction_count_sum=(\"transaction_count\", \"sum\"),\n",
    "    transaction_count_mean=(\"transaction_count\", \"mean\"),\n",
    "    transaction_sum_mean=(\"transaction_sum\", \"mean\"),\n",
    "    customer_count_sum=(\"customer_count\", \"sum\"),\n",
    "    customer_count_mean=(\"customer_count\", \"mean\")\n",
    ")\n",
    "\n",
    "# Na settings\n",
    "na_features_to_fill_with_mean = ['construction_year_max', 'construction_year_mean', 'construction_year_min', 'construction_year_max_city_relative', 'construction_year_mean_city_relative',  'construction_year_min_city_relative',\n",
    "'construction_year_max_neighbour_max', 'construction_year_mean_neighbour_max', 'construction_year_min_neighbour_max', 'construction_year_max_neighbour_mean', 'construction_year_mean_neighbour_mean',\n",
    "'construction_year_min_neighbour_mean']\n",
    "\n",
    "na_features_to_fill_with_median = ['price_meter_mean', 'price_meter_min', 'price_meter_mean_city_relative', 'price_meter_min_city_relative',  'price_meter_mean_neighbour_max', 'price_meter_min_neighbour_max']\n",
    "\n",
    "# Target column\n",
    "target_agg_column = \"transaction_sum\"\n",
    "\n",
    "# Kfold setting\n",
    "kfold_splits_settings = dict(\n",
    "    quantilies=10, \n",
    "    n_folds=5,\n",
    "    use_stratification=True\n",
    ")\n",
    "\n",
    "# This cols we won`t include in final graph\n",
    "cols2drop_from_final_graph = ['region_id',\n",
    "     'city_id',\n",
    "     'ldy',\n",
    "     'ldx',\n",
    "     'lty',\n",
    "     'ltx',\n",
    "     'rty',\n",
    "     'rtx',\n",
    "     'rdy',\n",
    "     'rdx',\n",
    "     'lat_centre',\n",
    "     'lon_centre',\n",
    "     'datep'\n",
    "]\n",
    "\n",
    "\n",
    "# Paths to save results\n",
    "path_to_save_graph = f\"../data/preprocessed_graphs/{regions_mapper[region]}/graph.pickle\"\n",
    "path_to_save_targets = f\"../data/preprocessed_graphs/{regions_mapper[region]}/graph_targets.parquet\"\n",
    "path_to_save_features = f\"../data/preprocessed_graphs/{regions_mapper[region]}/graph_features.parquet\"\n",
    "\n",
    "# Save results or not\n",
    "save_result = True\n",
    "\n",
    "# Features config\n",
    "# - add_business_features - not target bussiness transactions features (bool)\n",
    "# - metro_features - available only for some cities, state for some metro features (False/list of feature names)\n",
    "\n",
    "# features_config = dict(add_business_features=True, metro_features=[\"number of subways\", \"distance_to_nearest_metro\"])\n",
    "features_config = dict(add_business_features=True, metro_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "640ad56e-1582-4386-9e60-fee29519efc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:25:13.742380Z",
     "iopub.status.busy": "2024-07-21T10:25:13.741464Z",
     "iopub.status.idle": "2024-07-21T10:25:13.757137Z",
     "shell.execute_reply": "2024-07-21T10:25:13.756368Z",
     "shell.execute_reply.started": "2024-07-21T10:25:13.742340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(x, train_size, val_size):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    x = x.sample(frac=1)\n",
    "    \n",
    "    train_actual_size = int(x.shape[0] * train_size)\n",
    "    val_actual_size = int(x.shape[0] * val_size) + train_actual_size\n",
    "    train = x.iloc[:train_actual_size, :]\n",
    "    val = x.iloc[train_actual_size:val_actual_size + 1, :]\n",
    "    test = x.iloc[val_actual_size + 1:, :]\n",
    "    \n",
    "    return list(train.index), list(val.index), list(test.index)\n",
    "\n",
    "def split_folds(data, target_column, quantilies=10, n_folds=5, use_stratification=False):\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    train_folds = []\n",
    "    val_folds = []\n",
    "    \n",
    "    if use_stratification:\n",
    "    \n",
    "        data_copy[\"stratified_target\"] = pd.qcut(data_copy[target_column], q=quantilies, labels=np.arange(quantilies))\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        for i, (train_ind, val_ind) in enumerate(skf.split(data_copy, data_copy[\"stratified_target\"])):\n",
    "            \n",
    "            train_folds.append(data_copy.iloc[train_ind, :].index)\n",
    "            val_folds.append(data_copy.iloc[val_ind, :].index)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        for i, (train_ind, val_ind) in enumerate(kf.split(data_copy)):\n",
    "            \n",
    "            train_folds.append(data_copy.iloc[train_ind, :].index)\n",
    "            val_folds.append(data_copy.iloc[val_ind, :].index)\n",
    "            \n",
    "    return train_folds, val_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0eb2d6bf-1437-4a5d-9ff7-4802a8ebe749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:34:07.066134Z",
     "iopub.status.busy": "2024-07-21T10:34:07.065239Z",
     "iopub.status.idle": "2024-07-21T10:34:07.100099Z",
     "shell.execute_reply": "2024-07-21T10:34:07.099244Z",
     "shell.execute_reply.started": "2024-07-21T10:34:07.066091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_graph():\n",
    "    \n",
    "    \n",
    "    #loading data\n",
    "    features2keep = []\n",
    "    \n",
    "    print(\"Staring loading data...\", end=\"\\n\")\n",
    "    features = pd.read_parquet(\n",
    "        os.path.join(\n",
    "            root_data_path, \n",
    "            root_features_path, \n",
    "            f\"geo_features_{region}_buffer_5.parquet\"\n",
    "        )\n",
    "    )\n",
    "                               \n",
    "    features2keep.extend(\n",
    "        list([col for col in features.columns if col != zone_id_column])\n",
    "    )\n",
    "    \n",
    "                               \n",
    "    transactions = pd.read_parquet(\n",
    "        os.path.join(root_data_path, root_transaction_path, f\"transaction_data_{region}_buffer_5.parquet\")\n",
    "    )[transaction_features + [zone_id_column]]\n",
    "                               \n",
    "    with open(os.path.join(root_graphs_path, f\"graph_{region}_buffer_5_road_connected.gpickle\"), 'rb') as f:\n",
    "        Graph = pickle.load(f)\n",
    "    \n",
    "    print(\"Finished loading data...\", end=\"\\n\")\n",
    "    \n",
    "\n",
    "    #adding new features\n",
    "    if features_config[\"add_business_features\"]:\n",
    "        \n",
    "        print(\"Staring adding business features...\", end=\"\\n\")\n",
    "        \n",
    "        #selecting all businesses except for target\n",
    "        bs_types = [x for x in transactions['industry_type'].unique() if x != target_business]\n",
    "        \n",
    "        #creating new df for this features\n",
    "        bs_features = pd.DataFrame({zone_id_column : transactions.zone_id.unique()})\n",
    "                               \n",
    "        #creating features\n",
    "        for bs in tqdm(bs_types):\n",
    "                               \n",
    "            cur_bs_features = transactions[transactions.industry_type == bs].groupby(\n",
    "                zone_id_column, as_index=False\n",
    "            ).agg(**bussiness_features_agg)\n",
    "            \n",
    "            cur_bs_features.columns = [zone_id_column] + [f\"{bs}_{x}\" for x in cur_bs_features.columns[1:]]\n",
    "            features2keep.extend(cur_bs_features.columns[1:])\n",
    "            bs_features = bs_features.merge(cur_bs_features, on=zone_id_column, how=\"left\")\n",
    "\n",
    "        features = features.merge(bs_features, on=zone_id_column, how=\"left\")\n",
    "        \n",
    "        print(\"Finished adding business features...\", end=\"\\n\")\n",
    "        \n",
    "\n",
    "    #adding metro features (optionally, for some cities there are no Metro)\n",
    "    if features_config[\"metro_features\"]:\n",
    "        \n",
    "        print(\"Staring adding metro features...\", end=\"\\n\")\n",
    "        \n",
    "        metro_features = pd.read_csv(\n",
    "           os.path.join(root_metro_features_path, f\"subway_{region}_features.csv\")\n",
    "        )[[zone_id_column] + features_config[\"metro_features\"]]\n",
    "        \n",
    "        features = features.merge(metro_features, on=zone_id_column, how=\"left\")\n",
    "\n",
    "        features2keep.extend(features_config[\"metro_features\"])\n",
    "        \n",
    "        print(\"Finished adding metro features...\", end=\"\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Staring adding features oh the graph\", end=\"\\n\")\n",
    "    #aggregating everything\n",
    "    nodes_features = pd.DataFrame({zone_id_column : list(Graph.nodes())})\n",
    "        \n",
    "    nodes_features = nodes_features[[zone_id_column]].merge(\n",
    "        features[[zone_id_column] + features2keep],  on=zone_id_column, how='left',\n",
    "    ).set_index(zone_id_column)\n",
    "        \n",
    "    nodes_features[zone_id_column] = nodes_features.index\n",
    "    \n",
    "    #filling up na`s\n",
    "    for col in na_features_to_fill_with_mean:\n",
    "        nodes_features[col] = SimpleImputer(\n",
    "            strategy=\"mean\"\n",
    "        ).fit_transform(nodes_features[col].values.reshape(-1,1))\n",
    "\n",
    "    for col in na_features_to_fill_with_median:\n",
    "        nodes_features[col] = SimpleImputer(\n",
    "            strategy=\"median\"\n",
    "        ).fit_transform(nodes_features[col].values.reshape(-1,1))\n",
    "\n",
    "    nodes_features.fillna(-1, inplace=True)\n",
    "\n",
    "    #setting features to graph\n",
    "    nx.set_node_attributes(Graph, nodes_features.to_dict(orient=\"index\"))\n",
    "    \n",
    "    print(\"Finished adding features oh the graph\", end=\"\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Staring creating target and masks\", end=\"\\n\")\n",
    "    #forming target\n",
    "    target_transactions = transactions[transactions.industry_type == target_business]\n",
    "    targets = target_transactions.groupby(\n",
    "        zone_id_column, as_index=False\n",
    "    )[target_agg_column].agg(\"sum\").set_index(zone_id_column)\n",
    "    \n",
    "    #default split\n",
    "    tar_train, tar_val, tar_test = train_test_split(x=targets, train_size=0.7, val_size=0.15)\n",
    "    \n",
    "    #split folds\n",
    "    train_folds, val_folds = split_folds(\n",
    "        data=targets, \n",
    "        target_column=target_agg_column, \n",
    "        **kfold_splits_settings\n",
    "    )\n",
    "    \n",
    "    #df with masks\n",
    "    targets_out = pd.DataFrame(\n",
    "        {\n",
    "            zone_id_column : list(Graph.nodes()), \n",
    "            \"y\" : [0] * len(list(Graph.nodes())),\n",
    "        }\n",
    "    ).set_index(zone_id_column)\n",
    "\n",
    "    #setting target to all nodes\n",
    "    for i in list(Graph.nodes()):\n",
    "        if i in targets.index: targets_out.loc[i, \"y\"] = targets.loc[i, target_agg_column]\n",
    "        else: targets_out.loc[i, \"y\"] = -1\n",
    "\n",
    "    #creating mask for deafault splits\n",
    "    targets_masks = pd.DataFrame({\n",
    "        zone_id_column : list(Graph.nodes()), \n",
    "        \"train_mask\" : list(Graph.nodes()),\n",
    "        \"val_mask\" : list(Graph.nodes()),\n",
    "        \"test_mask\" : list(Graph.nodes()),\n",
    "        \"train_val_mask\" : list(Graph.nodes()),\n",
    "    }).set_index(zone_id_column).isin(\n",
    "        {\n",
    "            \"train_mask\" : tar_train, \n",
    "            \"val_mask\" : tar_val, \n",
    "            \"test_mask\" : tar_test, \n",
    "            \"train_val_mask\" : tar_train + tar_val\n",
    "        }\n",
    "    ).reset_index(drop=False)\n",
    "\n",
    "    #creating mask for kfold splits\n",
    "    k_folds_mask_df = pd.DataFrame({zone_id_column : list(Graph.nodes()),})\n",
    "    isin_dict = {}\n",
    "\n",
    "    for i, (train_fold_idx, val_fold_idx) in enumerate(zip(train_folds, val_folds)):\n",
    "        k_folds_mask_df[f\"train_fold_{i}\"] = list(Graph.nodes())\n",
    "        k_folds_mask_df[f\"val_fold_{i}\"] = list(Graph.nodes())\n",
    "        isin_dict[f\"train_fold_{i}\"] = train_fold_idx\n",
    "        isin_dict[f\"val_fold_{i}\"] = val_fold_idx\n",
    "\n",
    "    k_folds_mask_df = k_folds_mask_df.set_index(zone_id_column).isin(isin_dict).reset_index(drop=False)\n",
    "\n",
    "    targets_out = targets_out.merge(targets_masks, on=zone_id_column, how=\"left\").set_index(zone_id_column)\n",
    "    targets_out = targets_out.merge(k_folds_mask_df, on=zone_id_column, how=\"left\").set_index(zone_id_column)\n",
    "    \n",
    "    #setting masks to graph\n",
    "    nx.set_node_attributes(Graph, targets_out.to_dict(orient=\"index\"))\n",
    "    print(\"Finished creating target and masks\", end=\"\\n\")\n",
    "    \n",
    "    print(\"Staring creating final graph\", end=\"\\n\")\n",
    "    # creating torch_geometric.Data class\n",
    "    to_keep_as_single = [col for col in k_folds_mask_df.columns if \"fold\" in col] + \\\n",
    "    [\"y\", \"train_mask\", \"val_mask\", \"test_mask\", \"train_val_mask\", \"geometry\", zone_id_column]\n",
    "                               \n",
    "    g_data = from_networkx(Graph,\n",
    "                            group_node_attrs=[\n",
    "                                x for x in list(next(iter(Graph.nodes(data=True)))[-1].keys())\n",
    "                                if (x not in cols2drop_from_final_graph + to_keep_as_single)\n",
    "                            ]\n",
    "                          )\n",
    "    print(\"Finished creating final graph\", end=\"\\n\")\n",
    "                               \n",
    "    for col2drop in cols2drop_from_final_graph: \n",
    "        try: setattr(g_data, col2drop, None)\n",
    "        except: continue\n",
    "\n",
    "    if save_result:\n",
    "        print(\"Saving...\", end=\"\\n\")\n",
    "        torch.save(g_data, path_to_save_graph)\n",
    "        targets_out.to_parquet(path_to_save_targets, index=False)\n",
    "        nodes_features.drop(columns=cols2drop).to_parquet(path_to_save_features, index=False)\n",
    "        print(\"Saving finished succesfully!\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eacfe323-8220-489f-9e0e-094b9870d8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:34:07.768725Z",
     "iopub.status.busy": "2024-07-21T10:34:07.767422Z",
     "iopub.status.idle": "2024-07-21T10:34:07.781220Z",
     "shell.execute_reply": "2024-07-21T10:34:07.780267Z",
     "shell.execute_reply.started": "2024-07-21T10:34:07.768682Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# targets_out.to_csv(\"../data/zones_mask_473.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58c0ded4-73a2-468e-a3ff-11c0daa84428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T10:34:07.946176Z",
     "iopub.status.busy": "2024-07-21T10:34:07.945031Z",
     "iopub.status.idle": "2024-07-21T10:34:15.146559Z",
     "shell.execute_reply": "2024-07-21T10:34:15.145694Z",
     "shell.execute_reply.started": "2024-07-21T10:34:07.946127Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring loading data...\n",
      "Finished loading data...\n",
      "Staring adding business features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 99.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding business features...\n",
      "Staring adding features oh the graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding features oh the graph\n",
      "Staring creating target and masks\n",
      "Finished creating target and masks\n",
      "Staring creating final graph\n",
      "Finished creating final graph\n",
      "Saving...\n",
      "Saving finished succesfully!\n"
     ]
    }
   ],
   "source": [
    "cols = preprocess_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78704e75-5476-40ca-9d1e-58c01c377de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
